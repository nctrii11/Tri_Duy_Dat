{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd843efa",
   "metadata": {},
   "source": [
    "# Train Titanic models (converted from train_titanic.py)\n",
    "\n",
    "This notebook is a cell-by-cell conversion of `train_titanic.py`. It removes the CLI (`argparse`) bits and exposes parameters as variables so you can run, iterate, and inspect intermediate results interactively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcabf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and seed\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing (copied from script)\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_title(name):\n",
    "    m = re.search(r\",\\s*([^\\.]+)\\.\", name)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def preprocess(df, fit_objects=None):\n",
    "    # work on a copy\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop columns with too many missing values or not useful\n",
    "    for c in [\"Cabin\", \"Ticket\"]:\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=[c], inplace=True)\n",
    "\n",
    "    # Extract Title from Name\n",
    "    if \"Name\" in df.columns:\n",
    "        df[\"Title\"] = df[\"Name\"].apply(extract_title)\n",
    "        # simplify titles\n",
    "        df[\"Title\"] = df[\"Title\"].replace([\"Mlle\", \"Ms\"], \"Miss\")\n",
    "        df[\"Title\"] = df[\"Title\"].replace([\"Mme\"], \"Mrs\")\n",
    "        rare_titles = [\n",
    "            \"Lady\",\n",
    "            \"Countess\",\n",
    "            \"Capt\",\n",
    "            \"Col\",\n",
    "            \"Don\",\n",
    "            \"Dr\",\n",
    "            \"Major\",\n",
    "            \"Rev\",\n",
    "            \"Sir\",\n",
    "            \"Jonkheer\",\n",
    "            \"Dona\",\n",
    "        ]\n",
    "        df[\"Title\"] = df[\"Title\"].apply(lambda x: \"Rare\" if x in rare_titles else x)\n",
    "        df.drop(columns=[\"Name\"], inplace=True)\n",
    "\n",
    "    # Fill Embarked\n",
    "    if \"Embarked\" in df.columns:\n",
    "        if fit_objects and \"embarked_mode\" in fit_objects:\n",
    "            mode = fit_objects[\"embarked_mode\"]\n",
    "        else:\n",
    "            mode = df[\"Embarked\"].mode().iloc[0]\n",
    "        df[\"Embarked\"] = df[\"Embarked\"].fillna(mode)\n",
    "\n",
    "    # Fill Fare\n",
    "    if \"Fare\" in df.columns:\n",
    "        if fit_objects and \"fare_median\" in fit_objects:\n",
    "            fare_med = fit_objects[\"fare_median\"]\n",
    "        else:\n",
    "            fare_med = df[\"Fare\"].median()\n",
    "        df[\"Fare\"] = df[\"Fare\"].fillna(fare_med)\n",
    "\n",
    "    # Fill Age using median by Title when possible\n",
    "    if \"Age\" in df.columns:\n",
    "        if fit_objects and \"age_medians\" in fit_objects:\n",
    "            age_map = fit_objects[\"age_medians\"]\n",
    "            df[\"Age\"] = df.apply(\n",
    "                lambda r: (\n",
    "                    age_map.get(r[\"Title\"], age_map.get(\"overall\"))\n",
    "                    if pd.isnull(r[\"Age\"])\n",
    "                    else r[\"Age\"]\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "        else:\n",
    "            age_medians = df.groupby(\"Title\")[\"Age\"].median().to_dict()\n",
    "            overall = df[\"Age\"].median()\n",
    "            age_medians[\"overall\"] = overall\n",
    "            df[\"Age\"] = df.apply(\n",
    "                lambda r: (\n",
    "                    age_medians.get(r[\"Title\"], overall)\n",
    "                    if pd.isnull(r[\"Age\"])\n",
    "                    else r[\"Age\"]\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "    # Sex encoding\n",
    "    if \"Sex\" in df.columns:\n",
    "        df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1}).astype(int)\n",
    "\n",
    "    # One-hot Embarked and Title\n",
    "    cats = []\n",
    "    if \"Embarked\" in df.columns:\n",
    "        cats += [\"Embarked\"]\n",
    "    if \"Title\" in df.columns:\n",
    "        cats += [\"Title\"]\n",
    "\n",
    "    df = pd.get_dummies(df, columns=cats, drop_first=True)\n",
    "\n",
    "    # Drop PassengerId when present but keep index for submissions\n",
    "    if \"PassengerId\" in df.columns:\n",
    "        pid = df[\"PassengerId\"]\n",
    "        df.drop(columns=[\"PassengerId\"], inplace=True)\n",
    "    else:\n",
    "        pid = None\n",
    "\n",
    "    # Select features\n",
    "    feature_cols = [c for c in df.columns if c != \"Survived\"]\n",
    "\n",
    "    # Scale numeric features\n",
    "    num_cols = [c for c in [\"Age\", \"SibSp\", \"Parch\", \"Fare\"] if c in df.columns]\n",
    "    scaler = None\n",
    "    if fit_objects and \"scaler\" in fit_objects:\n",
    "        scaler = fit_objects[\"scaler\"]\n",
    "        df[num_cols] = scaler.transform(df[num_cols])\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        if num_cols:\n",
    "            df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "    fit_objects_out = {\"scaler\": scaler, \"feature_columns\": feature_cols}\n",
    "    return df, pid, fit_objects_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and logging functions\n",
    "def write_log(path, row):\n",
    "    header = [\n",
    "        \"timestamp\",\n",
    "        \"seed\",\n",
    "        \"model\",\n",
    "        \"accuracy\",\n",
    "        \"f1\",\n",
    "        \"roc_auc\",\n",
    "        \"runtime_s\",\n",
    "        \"artifact\",\n",
    "    ]\n",
    "    exists = os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        if not exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "def train_and_log(\n",
    "    train_csv_path, models_dir=\"models\", log_path=\"training_log.csv\", quick=False\n",
    "):\n",
    "    set_seed()\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    df = load_data(train_csv_path)\n",
    "    if \"Survived\" not in df.columns:\n",
    "        raise ValueError(\"train data must contain Survived column\")\n",
    "\n",
    "    # initial preprocess to compute fill values\n",
    "    df_proc, pid, fit_objs = preprocess(df)\n",
    "\n",
    "    X = df_proc.drop(columns=[\"Survived\"])\n",
    "    y = df_proc[\"Survived\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Models to train\n",
    "    models = {\n",
    "        \"logreg\": LogisticRegression(max_iter=500, random_state=SEED),\n",
    "        \"rf\": RandomForestClassifier(\n",
    "            n_estimators=100 if not quick else 10, random_state=SEED\n",
    "        ),\n",
    "        \"gb\": GradientBoostingClassifier(random_state=SEED),\n",
    "        \"svc\": SVC(probability=True, random_state=SEED),\n",
    "    }\n",
    "\n",
    "    # optional xgboost\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "\n",
    "        models[\"xgb\"] = XGBClassifier(\n",
    "            use_label_encoder=False, eval_metric=\"logloss\", random_state=SEED\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        dt = time.time() - t0\n",
    "        preds = model.predict(X_val)\n",
    "        proba = (\n",
    "            model.predict_proba(X_val)[:, 1]\n",
    "            if hasattr(model, \"predict_proba\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        acc = accuracy_score(y_val, preds)\n",
    "        f1 = f1_score(y_val, preds)\n",
    "        roc = roc_auc_score(y_val, proba) if proba is not None else float(\"nan\")\n",
    "\n",
    "        model_artifact = os.path.join(models_dir, f\"{name}.joblib\")\n",
    "        # save model and preprocessing objects\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"scaler\": fit_objs[\"scaler\"],\n",
    "                \"features\": fit_objs[\"feature_columns\"],\n",
    "            },\n",
    "            model_artifact,\n",
    "        )\n",
    "\n",
    "        # Append to CSV log\n",
    "        row = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"seed\": SEED,\n",
    "            \"model\": name,\n",
    "            \"accuracy\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"roc_auc\": roc,\n",
    "            \"runtime_s\": round(dt, 3),\n",
    "            \"artifact\": model_artifact,\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "        write_log(log_path, row)\n",
    "        print(\n",
    "            f\"Trained {name}: acc={acc:.4f}, f1={f1:.4f}, roc_auc={roc:.4f}, time={dt:.2f}s\"\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62215da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed: 42\n",
      "Loaded EDA\\train.csv\n",
      "After preprocess, shape: (891, 14)\n",
      "Feature columns (sample): ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Title_Miss', 'Title_Mr']\n"
     ]
    }
   ],
   "source": [
    "# Parameters and dry-run example\n",
    "DATA_PATH = os.path.join(\"EDA\", \"train.csv\")\n",
    "MODELS_DIR = \"models\"\n",
    "LOG_PATH = \"training_log.csv\"\n",
    "QUICK = True  # set to False for full training\n",
    "\n",
    "print(\"Using seed:\", SEED)\n",
    "# Dry run: load and preprocess only\n",
    "df = load_data(DATA_PATH)\n",
    "proc, pid, fit = preprocess(df)\n",
    "print(\"Loaded\", DATA_PATH)\n",
    "print(\"After preprocess, shape:\", proc.shape)\n",
    "print(\"Feature columns (sample):\", fit[\"feature_columns\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21eed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained logreg: acc=0.8156, f1=0.7519, roc_auc=0.8746, time=0.01s\n",
      "Trained rf: acc=0.7989, f1=0.7273, roc_auc=0.8170, time=0.01s\n",
      "Trained gb: acc=0.8101, f1=0.7258, roc_auc=0.8528, time=0.07s\n",
      "Trained svc: acc=0.8380, f1=0.7820, roc_auc=0.8466, time=0.04s\n",
      "Trained svc: acc=0.8380, f1=0.7820, roc_auc=0.8466, time=0.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\congt\\AppData\\Local\\Temp\\ipykernel_7980\\3409674776.py:90: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\congt\\AppData\\Local\\Temp\\ipykernel_7980\\3409674776.py:90: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\congt\\AppData\\Local\\Temp\\ipykernel_7980\\3409674776.py:90: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\congt\\AppData\\Local\\Temp\\ipykernel_7980\\3409674776.py:90: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>seed</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>runtime_s</th>\n",
       "      <th>artifact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-27T09:41:45.336004</td>\n",
       "      <td>42</td>\n",
       "      <td>logreg</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.874572</td>\n",
       "      <td>0.009</td>\n",
       "      <td>models\\logreg.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-27T09:41:45.354798</td>\n",
       "      <td>42</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.816996</td>\n",
       "      <td>0.010</td>\n",
       "      <td>models\\rf.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-27T09:41:45.434350</td>\n",
       "      <td>42</td>\n",
       "      <td>gb</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>0.852767</td>\n",
       "      <td>0.072</td>\n",
       "      <td>models\\gb.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-27T09:41:45.484308</td>\n",
       "      <td>42</td>\n",
       "      <td>svc</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.781955</td>\n",
       "      <td>0.846640</td>\n",
       "      <td>0.040</td>\n",
       "      <td>models\\svc.joblib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp  seed   model  accuracy        f1   roc_auc  \\\n",
       "0  2025-10-27T09:41:45.336004    42  logreg  0.815642  0.751880  0.874572   \n",
       "1  2025-10-27T09:41:45.354798    42      rf  0.798883  0.727273  0.816996   \n",
       "2  2025-10-27T09:41:45.434350    42      gb  0.810056  0.725806  0.852767   \n",
       "3  2025-10-27T09:41:45.484308    42     svc  0.837989  0.781955  0.846640   \n",
       "\n",
       "   runtime_s              artifact  \n",
       "0      0.009  models\\logreg.joblib  \n",
       "1      0.010      models\\rf.joblib  \n",
       "2      0.072      models\\gb.joblib  \n",
       "3      0.040     models\\svc.joblib  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training (this will train several scikit-learn baselines)\n",
    "results = train_and_log(\n",
    "    DATA_PATH, models_dir=MODELS_DIR, log_path=LOG_PATH, quick=QUICK\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3ad4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded artifact keys: ['model', 'scaler', 'features']\n",
      "Sample predictions: [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Inspect saved model artifacts and load one model to test\n",
    "import glob\n",
    "\n",
    "# ensure models dir exists and is a directory\n",
    "if not os.path.exists(MODELS_DIR) or not os.path.isdir(MODELS_DIR):\n",
    "    print(f\"Models directory '{MODELS_DIR}' does not exist or is not a directory.\")\n",
    "    artifacts = []\n",
    "else:\n",
    "    # search for joblib files inside models dir\n",
    "    artifacts = glob.glob(os.path.join(MODELS_DIR, \"*.joblib\"))\n",
    "\n",
    "artifacts\n",
    "\n",
    "if artifacts:\n",
    "    sample = joblib.load(artifacts[0])\n",
    "    print(\"Loaded artifact keys:\", list(sample.keys()))\n",
    "    # example: run inference on first 5 rows of validation-like data\n",
    "    df_small = proc.drop(columns=[\"Survived\"]).head(5)\n",
    "    # ensure numeric columns scaled using saved scaler\n",
    "    num_cols = [c for c in [\"Age\", \"SibSp\", \"Parch\", \"Fare\"] if c in df_small.columns]\n",
    "    if num_cols:\n",
    "        df_small[num_cols] = sample[\"scaler\"].transform(df_small[num_cols])\n",
    "    X_small = df_small[sample[\"features\"]]\n",
    "    preds = sample[\"model\"].predict(X_small)\n",
    "    print(\"Sample predictions:\", preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33895d",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- The notebook keeps the original preprocessing and training logic.\n",
    "- Interactive iteration tips: set `QUICK=False` for real training; increase `n_estimators` in the RandomForest model if desired.\n",
    "- If you want to re-create the exact CLI behavior, we can add a cell that parses `sys.argv` or use `argparse`-like helpers for notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
